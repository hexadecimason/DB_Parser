## Databse parsing pipeline

#### Cleaning

###### Steps

1) Filter APIs

Certain wells do not have an API, or have various comments in the 'API' field for the CSV file. A summary of these comments is generated by API_analyze.py. These wells will be filtered out: the scope of creating a PgSQL databse will involve being able to query on API, so wells without an API are of little use the vast majority of the time. Additionally, internal use of wells with no API is rare.

2) Filter "A/C" Files

Some 'File #' entries are equal to "A/C," indicating that they must be physically reassigned from the Amoco Collection (see "Comments" entires). These entries will be omitted: they do not contain any box-level data and the fact they still exist in the Amoco collection indicates they are rarely, if ever, pulled for client requests.

3) Expand box-level data

Many wells do not have individual boxes. These present in two ways:

	3.1) 'Box #' contains the total boxes, and the total is empty
	3.2) 'Box #' contains a range

To solve this, we will add to the "cleaned" DF row-by-row, creating new rows as appropriate.

#### Parsing

The next stage will be taking a cleaned CSV and restructuring it before it is entered into Postgres. This will involve creating abstract structures for "Well" data (OPIC_well.py object class), each instance of which contains an array of "Box" objects (OPIC_WellBox.py).

#### Postgres creation

psycopg2 is a python library designed to perform SQL commands. "db_test_code.sql" contains SQL commands for structuring a database and we will use this as a guide for how to use psycopg2 once cleaned data has been restructured as python objects.